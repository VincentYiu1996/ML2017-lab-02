{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import numpy.linalg as nl\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "x_train,y_train = load_svmlight_file(\"a9a\")\n",
    "x_test, y_test = load_svmlight_file('a9a.t')\n",
    "for i in range(0, len(y_train)):\n",
    "    if y_train[i]==-1:\n",
    "        y_train[i]=0\n",
    "for i in range(0,len(y_test)):\n",
    "    if y_test[i]==-1:\n",
    "        y_test[i]=0\n",
    "        \n",
    "\n",
    "temp =(np.ones((x_train.shape[0],1)))\n",
    "x_train=np.c_[x_train.A,temp]\n",
    "x_test=np.c_[x_test.A,np.zeros((x_test.shape[0],1)),np.ones((x_test.shape[0],1))]\n",
    "# y_train=y_train.reshape(len(y_train),-1)\n",
    "#初始化全零\n",
    "w1=np.zeros(x_train.shape[1])\n",
    "w2=w1\n",
    "w3=w1\n",
    "w4=w1\n",
    "\n",
    "\n",
    "\n",
    "#迭代次数\n",
    "num=500\n",
    "#阀值\n",
    "threshold=0.5\n",
    "\n",
    "loss_test={'NAG':[],'RMSProp':[],'AdaDelta':[],'Adam':[]}\n",
    "\n",
    "#目标函数\n",
    "def target(x,w):\n",
    "    return 1/(1+np.exp(-w.dot(x)))\n",
    "\n",
    "#loss函数\n",
    "def loss(x,y,w):\n",
    "    temp=0\n",
    "    for i in range(len(x)):\n",
    "        temp+=y[i] * np.log(target(x[i],w)) + (1 - y[i]) * np.log(1 - target(x[i],w))\n",
    "    return -temp/len(x)\n",
    "\n",
    "#求梯度\n",
    "def gradient(x,y,w):\n",
    "    g=0\n",
    "    #随机选取样本\n",
    "    rand=random.sample(range(len(x)),200)\n",
    "    for i in rand:\n",
    "        g+=(target(x[i],w)-y[i])*x[i]\n",
    "    return g/len(rand)\n",
    "\n",
    "#NAG\n",
    "def NAG(w):\n",
    "    V=0\n",
    "    learning_rate=0.01\n",
    "    y=0.9\n",
    "    for i in range(num):\n",
    "        grad=gradient(x_train,y_train,w-y*V)\n",
    "        V=y*V+learning_rate*grad\n",
    "        w=w-V\n",
    "        loss_test['NAG'].append(loss(x_test,y_test,w))\n",
    "        if(i%100==0):\n",
    "            print(i)\n",
    "\n",
    "    return w\n",
    "\n",
    "#RMSProp\n",
    "def RMSProp(w):\n",
    "    G=0\n",
    "    learning_rate=0.001\n",
    "    y=0.9\n",
    "    for i in range(num):\n",
    "        grad=gradient(x_train,y_train,w)\n",
    "        G=y*G+(1-y)*(grad*grad)\n",
    "        w=w-(learning_rate/np.sqrt(G+(1e-8))*grad)\n",
    "        loss_test['RMSProp'].append(loss(x_test,y_test,w))\n",
    "        if(i%100==0):\n",
    "            print(i)\n",
    "\n",
    "    return w\n",
    "\n",
    "#AdaDelta\n",
    "def AdaDelta(w):\n",
    "    G=0\n",
    "    delta_t=0\n",
    "    y=0.95\n",
    "    for i in range(num):\n",
    "        grad=gradient(x_train,y_train,w)\n",
    "        G=y*G+(1-y)*(grad*grad)\n",
    "        delta_w=grad*np.sqrt(delta_t+1e-8)/np.sqrt(G+1e-8)\n",
    "        w=w-delta_w\n",
    "        delta_t=y*delta_t+(1-y)*delta_w*delta_w\n",
    "        loss_test['AdaDelta'].append(loss(x_test,y_test,w))\n",
    "        if(i%100==0):\n",
    "            print(i)\n",
    "\n",
    "    return w\n",
    "\n",
    "def Adam(w):\n",
    "    b=0.9\n",
    "    m=0\n",
    "    G=0\n",
    "    learning_rate=0.001\n",
    "    y=0.999\n",
    "    for i in range(num):\n",
    "        grad=gradient(x_train,y_train,w)\n",
    "        m=b*m+(1-b)*grad\n",
    "        G=y*G+(1-y)*grad*grad\n",
    "        a=learning_rate*np.sqrt(1-y)/(1-b)\n",
    "        w=w-a*m/np.sqrt(G+1e-8)\n",
    "        loss_test['Adam'].append(loss(x_test,y_test,w))\n",
    "        if(i%100==0):\n",
    "            print(i)\n",
    "    return w\n",
    "    \n",
    "w1=NAG(w1)\n",
    "w2=RMSProp(w2)\n",
    "w3=AdaDelta(w3)\n",
    "w4=Adam(w4)\n",
    "#验证集命中率\n",
    "test_hit1=0\n",
    "test_hit2=0\n",
    "test_hit3=0\n",
    "test_hit4=0\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    if (np.dot(x_test[i],w1) >= threshold and y_test[i] == 1) or (np.dot(x_test[i],w1) < threshold and y_test[i] == 0):\n",
    "        test_hit1 += 1\n",
    "    if (np.dot(x_test[i],w2) >= threshold and y_test[i] == 1) or (np.dot(x_test[i],w2) < threshold and y_test[i] == 0):\n",
    "        test_hit2 += 1  \n",
    "    if (np.dot(x_test[i],w3) >= threshold and y_test[i] == 1) or (np.dot(x_test[i],w3) < threshold and y_test[i] == 0):\n",
    "        test_hit3 += 1\n",
    "    if (np.dot(x_test[i],w4) >= threshold and y_test[i] == 1) or (np.dot(x_test[i],w4) < threshold and y_test[i] == 0):\n",
    "        test_hit4 += 1\n",
    "\n",
    "print(test_hit1/len(x_test))\n",
    "print(test_hit2/len(x_test))\n",
    "print(test_hit3/len(x_test))\n",
    "print(test_hit4/len(x_test))\n",
    "\n",
    "n = np.arange(num)\n",
    "plt.plot(n,loss_test['NAG'],label='NAG')\n",
    "plt.plot(n,loss_test['RMSProp'],label='RMSProp')\n",
    "plt.plot(n,loss_test['AdaDelta'],label='AdaDelta')\n",
    "plt.plot(n,loss_test['Adam'],label='Adam')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Logistic Regression')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
